{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOhaKdwB+gBsMfdrTc7vt/5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jieshi1010/Claude-on-GCP/blob/main/jieshi_claude_1M_context_window_with_Vertex_Claude.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l3pL8yEsw_gC"
      },
      "source": [
        "# Using Claude with a 1M Context Window via Vertex AI\n",
        "\n",
        "This notebook demonstrates how to use Anthropic's Claude 3 model with a 1 million token context window through Google's Vertex AI. It covers setting up the environment, authenticating, and making an API call to the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kR4f8yZ7xGfA"
      },
      "source": [
        "## 1. Setup and Installation\n",
        "\n",
        "This section installs the necessary Python packages, including the Anthropic library with Vertex AI support. It also includes a step to restart the IPython kernel to ensure the newly installed packages are correctly loaded."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iMf8q2XSv4MP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "82e5f503-9473-4e3f-9a61-5648882ed52d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\\u001b[?25l   \\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\u001b[0m \\u001b[32m0.0/297.2 kB\\u001b[0m \\u001b[31m?\\u001b[0m eta \\u001b[36m-:--:--\\u001b[0m\\r\\u001b[2K   \\u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\u001b[0m\\u001b[91m╸\\u001b[0m \\u001b[32m297.0/297.2 kB\\u001b[0m \\u001b[31m9.4 MB/s\\u001b[0m eta \\u001b[36m0:00:01\\u001b[0m\\r\\u001b[2K   \\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\u001b[0m \\u001b[32m297.2/297.2 kB\\u001b[0m \\u001b[31m6.6 MB/s\\u001b[0m eta \\u001b[36m0:00:00\\u001b[0m\\n",
            "\\u001b[?25h"
          ]
        }
      ],
      "source": [
        "%pip install --upgrade --user --quiet 'anthropic[vertex]'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import IPython\n",
        "\n",
        "app = IPython.Application.instance()\n",
        "app.kernel.do_shutdown(True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M3reLKlpwPBS",
        "outputId": "f07c1187-1af9-41ee-af17-8fc2a2a03a1b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'status': 'ok', 'restart': True}"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZ4o4lB3xOqg"
      },
      "source": [
        "## 2. Authentication and Configuration\n",
        "\n",
        "This part of the notebook handles authentication with Google Cloud. If running in a Colab environment, it uses `google.colab.auth` to authenticate the user. It then sets up the project ID and location for the Vertex AI API calls."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "\n",
        "if \"google.colab\" in sys.modules:\n",
        "    from google.colab import auth\n",
        "\n",
        "    auth.authenticate_user()"
      ],
      "metadata": {
        "id": "Xys23vHcwQEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import datetime\n",
        "import time\n",
        "import base64\n",
        "import httpx\n",
        "\n",
        "PROJECT_ID = \"cloud-llm-preview1\"  # @param {type: \"string\", placeholder: \"[your-project-id]\", isTemplate: true}\n",
        "if not PROJECT_ID or PROJECT_ID == \"[your-project-id]\":\n",
        "    PROJECT_ID = str(os.environ.get(\"GOOGLE_CLOUD_PROJECT\"))\n",
        "\n",
        "LOCATION = os.environ.get(\"GOOGLE_CLOUD_REGION\", \"global\")\n"
      ],
      "metadata": {
        "id": "GTMzj9M2wZt3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJ1p3lDuxV5X"
      },
      "source": [
        "## 3. API Initialization and Call\n",
        "\n",
        "Here, we initialize the `AnthropicVertex` client with the project ID and region. We then define the model to be used and make a call to the `client.beta.messages.create` method. This example sends a simple prompt to the model and demonstrates how to enable the 1M context window using the `betas` parameter."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from anthropic import AnthropicVertex\n",
        "\n",
        "# Initialize the Anthropic Vertex client\n",
        "# This client is used to interact with the Anthropic models on Vertex AI.\n",
        "client = AnthropicVertex(region=REGION, project_id=PROJECT_ID)\n",
        "\n",
        "# Define the model to be used\n",
        "# In this case, we are using claude-sonnet-4, but you can replace it with other\n",
        "# available models like claude-opus-4.\n",
        "CLAUDE4 = \"claude-sonnet-4@20250514\""
      ],
      "metadata": {
        "id": "Gk7xVlZWAlyC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a message to send to the model\n",
        "# The `model` parameter specifies which version of Claude to use.\n",
        "# `max_tokens` sets the maximum number of tokens in the response.\n",
        "# `messages` contains the conversation history, starting with the user's prompt.\n",
        "# The `betas` parameter is used to enable beta features, in this case, the\n",
        "# 1M context window.\n",
        "response = client.beta.messages.create(\n",
        "    model=CLAUDE4,\n",
        "    max_tokens=1024,\n",
        "    messages=[\n",
        "        {\"role\": \"user\", \"content\": \"tell me a story about a cat\"}\n",
        "    ],\n",
        "    betas=[\"context-1m-2025-08-07\"]\n",
        ")"
      ],
      "metadata": {
        "id": "mm2qShqAwQ9R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b-4d4zJ4xZfJ"
      },
      "source": [
        "## 4. Displaying the Response\n",
        "\n",
        "The final step is to print the response received from the model. This will show the output of the story generated by Claude."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the response from the model\n",
        "# The `response` object contains the model's output, including the generated text.\n",
        "print(response)"
      ],
      "metadata": {
        "id": "qUMfk6aWxnC4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}